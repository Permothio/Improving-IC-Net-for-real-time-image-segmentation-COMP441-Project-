 {
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4ZW1jNDOBer",
        "outputId": "c1b332b2-5cba-4056-cbc8-b667f44fc52c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "59qjAU6_MaIj"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# File   : comm.py\n",
        "# Author : Jiayuan Mao\n",
        "# Email  : maojiayuan@gmail.com\n",
        "# Date   : 27/01/2018\n",
        "#\n",
        "# This file is part of Synchronized-BatchNorm-PyTorch.\n",
        "# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "# Distributed under MIT License.\n",
        "\n",
        "import queue\n",
        "import collections\n",
        "import threading\n",
        "\n",
        "__all__ = ['FutureResult', 'SlavePipe', 'SyncMaster']\n",
        "\n",
        "\n",
        "class FutureResult(object):\n",
        "    \"\"\"A thread-safe future implementation. Used only as one-to-one pipe.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._result = None\n",
        "        self._lock = threading.Lock()\n",
        "        self._cond = threading.Condition(self._lock)\n",
        "\n",
        "    def put(self, result):\n",
        "        with self._lock:\n",
        "            assert self._result is None, 'Previous result has\\'t been fetched.'\n",
        "            self._result = result\n",
        "            self._cond.notify()\n",
        "\n",
        "    def get(self):\n",
        "        with self._lock:\n",
        "            if self._result is None:\n",
        "                self._cond.wait()\n",
        "\n",
        "            res = self._result\n",
        "            self._result = None\n",
        "            return res\n",
        "\n",
        "\n",
        "_MasterRegistry = collections.namedtuple('MasterRegistry', ['result'])\n",
        "_SlavePipeBase = collections.namedtuple('_SlavePipeBase', ['identifier', 'queue', 'result'])\n",
        "\n",
        "\n",
        "class SlavePipe(_SlavePipeBase):\n",
        "    \"\"\"Pipe for master-slave communication.\"\"\"\n",
        "\n",
        "    def run_slave(self, msg):\n",
        "        self.queue.put((self.identifier, msg))\n",
        "        ret = self.result.get()\n",
        "        self.queue.put(True)\n",
        "        return ret\n",
        "\n",
        "\n",
        "class SyncMaster(object):\n",
        "    \"\"\"An abstract `SyncMaster` object.\n",
        "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
        "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
        "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
        "    and passed to a registered callback.\n",
        "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
        "    back to each slave devices.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, master_callback):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
        "        \"\"\"\n",
        "        self._master_callback = master_callback\n",
        "        self._queue = queue.Queue()\n",
        "        self._registry = collections.OrderedDict()\n",
        "        self._activated = False\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return {'master_callback': self._master_callback}\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        self.__init__(state['master_callback'])\n",
        "\n",
        "    def register_slave(self, identifier):\n",
        "        \"\"\"\n",
        "        Register an slave device.\n",
        "        Args:\n",
        "            identifier: an identifier, usually is the device id.\n",
        "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
        "        \"\"\"\n",
        "        if self._activated:\n",
        "            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n",
        "            self._activated = False\n",
        "            self._registry.clear()\n",
        "        future = FutureResult()\n",
        "        self._registry[identifier] = _MasterRegistry(future)\n",
        "        return SlavePipe(identifier, self._queue, future)\n",
        "\n",
        "    def run_master(self, master_msg):\n",
        "        \"\"\"\n",
        "        Main entry for the master device in each forward pass.\n",
        "        The messages were first collected from each devices (including the master device), and then\n",
        "        an callback will be invoked to compute the message to be sent back to each devices\n",
        "        (including the master device).\n",
        "        Args:\n",
        "            master_msg: the message that the master want to send to itself. This will be placed as the first\n",
        "            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n",
        "        Returns: the message to be sent back to the master device.\n",
        "        \"\"\"\n",
        "        self._activated = True\n",
        "\n",
        "        intermediates = [(0, master_msg)]\n",
        "        for i in range(self.nr_slaves):\n",
        "            intermediates.append(self._queue.get())\n",
        "\n",
        "        results = self._master_callback(intermediates)\n",
        "        assert results[0][0] == 0, 'The first result should belongs to the master.'\n",
        "\n",
        "        for i, res in results:\n",
        "            if i == 0:\n",
        "                continue\n",
        "            self._registry[i].result.put(res)\n",
        "\n",
        "        for i in range(self.nr_slaves):\n",
        "            assert self._queue.get() is True\n",
        "\n",
        "        return results[0][1]\n",
        "\n",
        "    @property\n",
        "    def nr_slaves(self):\n",
        "        return len(self._registry)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
        "\n",
        "__all__ = ['SynchronizedBatchNorm1d', 'SynchronizedBatchNorm2d', 'SynchronizedBatchNorm3d']\n",
        "\n",
        "\n",
        "def _sum_ft(tensor):\n",
        "    \"\"\"sum over the first and last dimention\"\"\"\n",
        "    return tensor.sum(dim=0).sum(dim=-1)\n",
        "\n",
        "\n",
        "def _unsqueeze_ft(tensor):\n",
        "    \"\"\"add new dementions at the front and the tail\"\"\"\n",
        "    return tensor.unsqueeze(0).unsqueeze(-1)\n",
        "\n",
        "\n",
        "_ChildMessage = collections.namedtuple('_ChildMessage', ['sum', 'ssum', 'sum_size'])\n",
        "_MasterMessage = collections.namedtuple('_MasterMessage', ['sum', 'inv_std'])\n",
        "\n",
        "\n",
        "class _SynchronizedBatchNorm(_BatchNorm):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
        "        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n",
        "\n",
        "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
        "\n",
        "        self._is_parallel = False\n",
        "        self._parallel_id = None\n",
        "        self._slave_pipe = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
        "        if not (self._is_parallel and self.training):\n",
        "            return F.batch_norm(\n",
        "                input, self.running_mean, self.running_var, self.weight, self.bias,\n",
        "                self.training, self.momentum, self.eps)\n",
        "\n",
        "        # Resize the input to (B, C, -1).\n",
        "        input_shape = input.size()\n",
        "        input = input.view(input.size(0), self.num_features, -1)\n",
        "\n",
        "        # Compute the sum and square-sum.\n",
        "        sum_size = input.size(0) * input.size(2)\n",
        "        input_sum = _sum_ft(input)\n",
        "        input_ssum = _sum_ft(input ** 2)\n",
        "\n",
        "        # Reduce-and-broadcast the statistics.\n",
        "        if self._parallel_id == 0:\n",
        "            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n",
        "        else:\n",
        "            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n",
        "\n",
        "        # Compute the output.\n",
        "        if self.affine:\n",
        "            # MJY:: Fuse the multiplication for speed.\n",
        "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n",
        "        else:\n",
        "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
        "\n",
        "        # Reshape it.\n",
        "        return output.view(input_shape)\n",
        "\n",
        "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
        "        self._is_parallel = True\n",
        "        self._parallel_id = copy_id\n",
        "\n",
        "        # parallel_id == 0 means master device.\n",
        "        if self._parallel_id == 0:\n",
        "            ctx.sync_master = self._sync_master\n",
        "        else:\n",
        "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
        "\n",
        "    def _data_parallel_master(self, intermediates):\n",
        "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
        "\n",
        "        # Always using same \"device order\" makes the ReduceAdd operation faster.\n",
        "        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n",
        "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
        "\n",
        "        to_reduce = [i[1][:2] for i in intermediates]\n",
        "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
        "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
        "\n",
        "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
        "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
        "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
        "\n",
        "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
        "\n",
        "        outputs = []\n",
        "        for i, rec in enumerate(intermediates):\n",
        "            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _compute_mean_std(self, sum_, ssum, size):\n",
        "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
        "        also maintains the moving average on the master device.\"\"\"\n",
        "        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n",
        "        mean = sum_ / size\n",
        "        sumvar = ssum - sum_ * mean\n",
        "        unbias_var = sumvar / (size - 1)\n",
        "        bias_var = sumvar / size\n",
        "\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n",
        "\n",
        "        return mean, bias_var.clamp(self.eps) ** -0.5\n",
        "\n",
        "\n",
        "class SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n",
        "    r\"\"\"Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n",
        "    mini-batch.\n",
        "    .. math::\n",
        "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
        "    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n",
        "    standard-deviation are reduced across all devices during training.\n",
        "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
        "    training, PyTorch's implementation normalize the tensor on each device using\n",
        "    the statistics only on that device, which accelerated the computation and\n",
        "    is also easy to implement, but the statistics might be inaccurate.\n",
        "    Instead, in this synchronized version, the statistics will be computed\n",
        "    over all training samples distributed on multiple devices.\n",
        "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
        "    as the built-in PyTorch implementation.\n",
        "    The mean and standard-deviation are calculated per-dimension over\n",
        "    the mini-batches and gamma and beta are learnable parameter vectors\n",
        "    of size C (where C is the input size).\n",
        "    During training, this layer keeps a running estimate of its computed mean\n",
        "    and variance. The running sum is kept with a default momentum of 0.1.\n",
        "    During evaluation, this running mean/variance is used for normalization.\n",
        "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
        "    on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
        "    Args:\n",
        "        num_features: num_features from an expected input of size\n",
        "            `batch_size x num_features [x width]`\n",
        "        eps: a value added to the denominator for numerical stability.\n",
        "            Default: 1e-5\n",
        "        momentum: the value used for the running_mean and running_var\n",
        "            computation. Default: 0.1\n",
        "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
        "            affine parameters. Default: ``True``\n",
        "    Shape:\n",
        "        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
        "        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
        "    Examples:\n",
        "        >>> # With Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm1d(100)\n",
        "        >>> # Without Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n",
        "        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n",
        "        >>> output = m(input)\n",
        "    \"\"\"\n",
        "\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 2 and input.dim() != 3:\n",
        "            raise ValueError('expected 2D or 3D input (got {}D input)'\n",
        "                             .format(input.dim()))\n",
        "        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n",
        "\n",
        "\n",
        "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
        "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
        "    of 3d inputs\n",
        "    .. math::\n",
        "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
        "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
        "    standard-deviation are reduced across all devices during training.\n",
        "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
        "    training, PyTorch's implementation normalize the tensor on each device using\n",
        "    the statistics only on that device, which accelerated the computation and\n",
        "    is also easy to implement, but the statistics might be inaccurate.\n",
        "    Instead, in this synchronized version, the statistics will be computed\n",
        "    over all training samples distributed on multiple devices.\n",
        "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
        "    as the built-in PyTorch implementation.\n",
        "    The mean and standard-deviation are calculated per-dimension over\n",
        "    the mini-batches and gamma and beta are learnable parameter vectors\n",
        "    of size C (where C is the input size).\n",
        "    During training, this layer keeps a running estimate of its computed mean\n",
        "    and variance. The running sum is kept with a default momentum of 0.1.\n",
        "    During evaluation, this running mean/variance is used for normalization.\n",
        "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
        "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
        "    Args:\n",
        "        num_features: num_features from an expected input of\n",
        "            size batch_size x num_features x height x width\n",
        "        eps: a value added to the denominator for numerical stability.\n",
        "            Default: 1e-5\n",
        "        momentum: the value used for the running_mean and running_var\n",
        "            computation. Default: 0.1\n",
        "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
        "            affine parameters. Default: ``True``\n",
        "    Shape:\n",
        "        - Input: :math:`(N, C, H, W)`\n",
        "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
        "    Examples:\n",
        "        >>> # With Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm2d(100)\n",
        "        >>> # Without Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
        "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
        "        >>> output = m(input)\n",
        "    \"\"\"\n",
        "\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 4:\n",
        "            raise ValueError('expected 4D input (got {}D input)'\n",
        "                             .format(input.dim()))\n",
        "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n",
        "\n",
        "\n",
        "class SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n",
        "    \"\"\"Applies Batch Normalization over a 5d input that is seen as a mini-batch\n",
        "    of 4d inputs\n",
        "    .. math::\n",
        "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
        "    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n",
        "    standard-deviation are reduced across all devices during training.\n",
        "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
        "    training, PyTorch's implementation normalize the tensor on each device using\n",
        "    the statistics only on that device, which accelerated the computation and\n",
        "    is also easy to implement, but the statistics might be inaccurate.\n",
        "    Instead, in this synchronized version, the statistics will be computed\n",
        "    over all training samples distributed on multiple devices.\n",
        "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
        "    as the built-in PyTorch implementation.\n",
        "    The mean and standard-deviation are calculated per-dimension over\n",
        "    the mini-batches and gamma and beta are learnable parameter vectors\n",
        "    of size C (where C is the input size).\n",
        "    During training, this layer keeps a running estimate of its computed mean\n",
        "    and variance. The running sum is kept with a default momentum of 0.1.\n",
        "    During evaluation, this running mean/variance is used for normalization.\n",
        "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
        "    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric BatchNorm\n",
        "    or Spatio-temporal BatchNorm\n",
        "    Args:\n",
        "        num_features: num_features from an expected input of\n",
        "            size batch_size x num_features x depth x height x width\n",
        "        eps: a value added to the denominator for numerical stability.\n",
        "            Default: 1e-5\n",
        "        momentum: the value used for the running_mean and running_var\n",
        "            computation. Default: 0.1\n",
        "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
        "            affine parameters. Default: ``True``\n",
        "    Shape:\n",
        "        - Input: :math:`(N, C, D, H, W)`\n",
        "        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n",
        "    Examples:\n",
        "        >>> # With Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm3d(100)\n",
        "        >>> # Without Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n",
        "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n",
        "        >>> output = m(input)\n",
        "    \"\"\"\n",
        "\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 5:\n",
        "            raise ValueError('expected 5D input (got {}D input)'\n",
        "                             .format(input.dim()))\n",
        "        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)"
      ],
      "metadata": {
        "id": "EZViz1SXOISO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "#from models.sync_batchnorm import SynchronizedBatchNorm2d\n",
        "\n",
        "def load_weights_sequential(target, source_state):\n",
        "    \n",
        "    new_dict = OrderedDict()\n",
        "\n",
        "    for k1, v1 in target.state_dict().items():\n",
        "        if not 'num_batches_tracked' in k1:\n",
        "            tar_v = source_state[k1]\n",
        "\n",
        "            if v1.shape != tar_v.shape:\n",
        "                c, _, w, h = v1.shape\n",
        "                tar_v = torch.cat([\n",
        "                    tar_v, \n",
        "                    torch.zeros((c,v1.shape[1]-tar_v.shape[1],w,h)),\n",
        "                ], 1)\n",
        "\n",
        "            new_dict[k1] = tar_v\n",
        "    target.load_state_dict(new_dict)\n",
        "\n",
        "model_urls = {\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, dilation=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, dilation=dilation, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride=stride, dilation=dilation)\n",
        "        self.bn1 = SynchronizedBatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes, stride=1, dilation=dilation)\n",
        "        self.bn2 = SynchronizedBatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = SynchronizedBatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, dilation=dilation,\n",
        "                               padding=dilation, bias=False)\n",
        "        self.bn2 = SynchronizedBatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = SynchronizedBatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers=(3, 4, 23, 3)):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        #self.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3,\n",
        "         #                      bias=False)\n",
        "        self.bn1 = SynchronizedBatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, SynchronizedBatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                SynchronizedBatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, dilation=dilation))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x): # [6, 4, 224, 224]\n",
        "        x = self.conv1(x)  # /2 [6, 64, 112, 112]\n",
        "        x = self.bn1(x)\n",
        "        x_1 = self.relu(x)\n",
        "        x = self.maxpool(x_1)  # /2 [6, 64, 56, 56]\n",
        "\n",
        "        x_2 = self.layer1(x) # [[6, 256, 56, 56]]\n",
        "        x = self.layer2(x_2)   # /2 [1, 512, 28, 28]\n",
        "        x_3 = self.layer3(x) # [1, 1024, 28, 28]\n",
        "        x = self.layer4(x_3) # [1, 2048, 28, 28]\n",
        "\n",
        "        return x_1, x_2, x_3\n",
        "\n",
        "\n",
        "def resnet50(pretrained=True):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3]) # [3, 4, 6, 3]\n",
        "    if pretrained:\n",
        "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet50']))\n",
        "\n",
        "    #model = base_model\n",
        "    \n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "jQLSRisTOI7-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n",
        "#from models.sync_batchnorm import SynchronizedBatchNorm2d\n",
        "\n",
        "class _ASPPModule(nn.Module):\n",
        "    def __init__(self, inplanes, planes, kernel_size, padding, dilation, BatchNorm):\n",
        "        super(_ASPPModule, self).__init__()\n",
        "        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
        "                                            stride=1, padding=padding, dilation=dilation, bias=False)\n",
        "        self.bn = BatchNorm(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self._init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.atrous_conv(x)\n",
        "        x = self.bn(x)\n",
        "\n",
        "        return self.relu(x)\n",
        "\n",
        "    def _init_weight(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, SynchronizedBatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "class ASPP_no4level(nn.Module):\n",
        "    def __init__(self, backbone, output_stride, BatchNorm):\n",
        "        super(ASPP_no4level, self).__init__()\n",
        "        if backbone == 'drn':\n",
        "            inplanes = 512\n",
        "        elif backbone == 'mobilenet':\n",
        "            inplanes = 320\n",
        "        else:\n",
        "            inplanes = 2048\n",
        "            low_level_inplanes = 256 #\n",
        "        if output_stride == 16:\n",
        "            dilations = [1, 6, 12, 18]\n",
        "        elif output_stride == 8:\n",
        "            dilations = [1, 12, 24, 36]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.aspp1_128 = _ASPPModule(64, 64, 1, padding=0, dilation=dilations[0], BatchNorm=BatchNorm)\n",
        "        self.aspp1_256 = _ASPPModule(256, 64, 1, padding=0, dilation=dilations[0], BatchNorm=BatchNorm)\n",
        "        self.aspp1_1024 = _ASPPModule(1024, 128, 1, padding=0, dilation=dilations[0], BatchNorm=BatchNorm)\n",
        "\n",
        "        self.bn1_128 = BatchNorm(64)\n",
        "        self.bn1_256 = BatchNorm(64)\n",
        "        self.bn1_1024 = BatchNorm(128)\n",
        "        # self.bn1_2048 = BatchNorm(256)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.last_conv = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                BatchNorm(256),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Dropout(0.5))\n",
        "\n",
        "        self._init_weight()\n",
        "        print(\"ASPP_4level\")\n",
        "\n",
        "    def forward(self, x_1, x_2, x_3):\n",
        "        x_1 = self.aspp1_128(x_1)\n",
        "        x_1 = self.bn1_128(x_1)\n",
        "        x_1 = self.relu(x_1)\n",
        "        x_1 = self.dropout(x_1)\n",
        "\n",
        "        x_2 = self.aspp1_256(x_2)\n",
        "        x_2 = self.bn1_256(x_2)\n",
        "        x_2 = self.relu(x_2)\n",
        "        x_2 = self.dropout(x_2)\n",
        "\n",
        "        x_3 = self.aspp1_1024(x_3)\n",
        "        x_3 = self.bn1_1024(x_3)\n",
        "        x_3 = self.relu(x_3)\n",
        "        x_3 = self.dropout(x_3)\n",
        "\n",
        "        x_2 = F.interpolate(x_2, size=x_1.size()[2:], mode='bilinear', align_corners=True)\n",
        "        x_3 = F.interpolate(x_3, size=x_1.size()[2:], mode='bilinear', align_corners=True)\n",
        "        x = torch.cat((x_1, x_2, x_3), dim=1)\n",
        "        x = self.last_conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_weight(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, SynchronizedBatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "metadata": {
        "id": "bqCY-KqaOKzJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#from models.network import extractors\n",
        "#from models.sync_batchnorm import SynchronizedBatchNorm2d\n",
        "#from models.network.aspp import ASPP_no4level\n",
        "\n",
        "def make_coord(shape, ranges=None, flatten=True):\n",
        "    \"\"\" Make coordinates at grid centers.\n",
        "    \"\"\"\n",
        "    coord_seqs = []\n",
        "    for i, n in enumerate(shape):\n",
        "        if ranges is None:\n",
        "            v0, v1 = -1, 1\n",
        "        else:\n",
        "            v0, v1 = ranges[i]\n",
        "        r = (v1 - v0) / (2 * n)\n",
        "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
        "        coord_seqs.append(seq)\n",
        "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
        "    if flatten:\n",
        "        ret = ret.view(-1, ret.shape[-1])\n",
        "    return ret\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, hidden_list):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        lastv = in_dim\n",
        "        for hidden in hidden_list:\n",
        "            layers.append(nn.Linear(lastv, hidden))\n",
        "            layers.append(nn.ReLU())\n",
        "            lastv = hidden\n",
        "        layers.append(nn.Linear(lastv, out_dim))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = x.shape[:-1]\n",
        "        x = self.layers(x.view(-1, x.shape[-1]))\n",
        "        return x.view(*shape, -1)\n",
        "\n",
        "\n",
        "class CRMNet(nn.Module):\n",
        "    def __init__(self,  backend='resnet34', pretrained=True):\n",
        "        super().__init__()\n",
        "        self.feats = resnet50(pretrained)\n",
        "        self.aspp_ = ASPP_no4level(backbone=backend, output_stride=8, BatchNorm=SynchronizedBatchNorm2d)\n",
        "        self.imnet = MLP(in_dim=256+6, out_dim=1, hidden_list=[32, 32, 32, 32])\n",
        "\n",
        "    def forward(self, x, seg, coord, cell, inter_s8=None, inter_s4=None):\n",
        " \n",
        "        # extract feature\n",
        "        p = torch.cat((x, seg), 1)\n",
        "        # x, low_level_feat\n",
        "        \n",
        "        x1_feat, x2_feat, x3_feat = self.feats(p) # [6, 64, 112, 112] [6, 256, 56, 56] [6, 1024, 28, 28]\n",
        "        feat = self.aspp_(x1_feat, x2_feat, x3_feat)\n",
        "        \n",
        "        vx_lst = [-1, 1]\n",
        "        vy_lst = [-1, 1]\n",
        "        eps_shift = 1e-6\n",
        "\n",
        "        rx = 2 / feat.shape[-2] / 2 \n",
        "        ry = 2 / feat.shape[-1] / 2 \n",
        "\n",
        "        feat_coord = make_coord(feat.shape[-2:], flatten=False).cuda().permute(2, 0, 1).unsqueeze(0).expand(feat.shape[0], 2, *feat.shape[-2:]) # \n",
        "\n",
        "        preds = []\n",
        "        areas = []\n",
        "        for vx in vx_lst:\n",
        "            for vy in vy_lst:\n",
        "                coord_ = coord.clone()\n",
        "                coord_[:, :, 0] += vx * rx + eps_shift\n",
        "                coord_[:, :, 1] += vy * ry + eps_shift\n",
        "                coord_.clamp_(-1 + 1e-6, 1 - 1e-6)\n",
        "                q_feat = F.grid_sample(\n",
        "                    feat, coord_.flip(-1).unsqueeze(1),\n",
        "                    mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
        "                    .permute(0, 2, 1)\n",
        "                q_coord = F.grid_sample(\n",
        "                    feat_coord, coord_.flip(-1).unsqueeze(1),\n",
        "                    mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
        "                    .permute(0, 2, 1)\n",
        "                rel_coord = coord - q_coord\n",
        "                rel_coord[:, :, 0] *= feat.shape[-2]\n",
        "                rel_coord[:, :, 1] *= feat.shape[-1]\n",
        "                inp = torch.cat([q_feat, rel_coord, coord], dim=-1)\n",
        "\n",
        "                # if self.cell_decode:\n",
        "                rel_cell = cell.clone()\n",
        "                rel_cell[:, :, 0] *= feat.shape[-2]\n",
        "                rel_cell[:, :, 1] *= feat.shape[-1]\n",
        "                inp = torch.cat([inp, rel_cell], dim=-1)\n",
        "\n",
        "                bs, q = coord.shape[:2]\n",
        "                pred = self.imnet(inp.view(bs * q, -1)).view(bs, q, -1)\n",
        "                preds.append(pred)\n",
        "\n",
        "                area = torch.abs(rel_coord[:, :, 0] * rel_coord[:, :, 1])\n",
        "                areas.append(area + 1e-9)\n",
        "\n",
        "        tot_area = torch.stack(areas).sum(dim=0)\n",
        "        # if self.local_ensemble:\n",
        "        t = areas[0]; areas[0] = areas[3]; areas[3] = t\n",
        "        t = areas[1]; areas[1] = areas[2]; areas[2] = t\n",
        "        ret = 0\n",
        "\n",
        "        for pred, area in zip(preds, areas):\n",
        "            ret = ret + pred * (area / tot_area).unsqueeze(-1)\n",
        "        \n",
        "        pred_224 = torch.sigmoid(ret) # [6, 1, 224, 224]\n",
        "\n",
        "        images = {}\n",
        "        images['out_224'] = ret\n",
        "        images['pred_224'] = pred_224\n",
        "        \n",
        "        return images    "
      ],
      "metadata": {
        "id": "MBNVRILROMzS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SobelOperator(nn.Module):\n",
        "    def __init__(self, epsilon):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        x_kernel = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])/4\n",
        "        self.conv_x = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv_x.weight.data = torch.tensor(x_kernel).unsqueeze(0).unsqueeze(0).float().cuda()\n",
        "        self.conv_x.weight.requires_grad = False\n",
        "\n",
        "        y_kernel = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])/4\n",
        "        self.conv_y = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv_y.weight.data = torch.tensor(y_kernel).unsqueeze(0).unsqueeze(0).float().cuda()\n",
        "        self.conv_y.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, c, h, w = x.shape\n",
        "        if c > 1:\n",
        "            x = x.view(b*c, 1, h, w)\n",
        "\n",
        "        x = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        grad_x = self.conv_x(x)\n",
        "        grad_y = self.conv_y(x)\n",
        "        \n",
        "        x = torch.sqrt(grad_x ** 2 + grad_y ** 2 + self.epsilon)\n",
        "\n",
        "        x = x.view(b, c, h, w)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SobelComputer:\n",
        "    def __init__(self):\n",
        "        self.sobel = SobelOperator(1e-4)\n",
        "\n",
        "    def compute_edges(self, images):\n",
        "        gt = images['gt'].float()\n",
        "        pred = images['pred_224'].float()\n",
        "        images['gt_sobel'] = self.sobel(gt)\n",
        "        images['pred_sobel'] = self.sobel(pred)"
      ],
      "metadata": {
        "id": "-cuNMuN_OOpV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# import git\n",
        "import warnings\n",
        "\n",
        "def tensor_to_numpy(image):\n",
        "    image_np = (image.numpy() * 255).astype('uint8')\n",
        "    return image_np\n",
        "\n",
        "def detach_to_cpu(x):\n",
        "    return x.detach().cpu()\n",
        "\n",
        "def fix_width_trunc(x):\n",
        "    return ('{:.9s}'.format('{:0.9f}'.format(x)))\n",
        "\n",
        "class BoardLogger:\n",
        "    def __init__(self, id):\n",
        "\n",
        "        if id is None:\n",
        "            self.no_log = True\n",
        "            warnings.warn('Logging has been disbaled.')\n",
        "        else:\n",
        "            self.no_log = False\n",
        "\n",
        "            self.inv_im_trans = transforms.Normalize(\n",
        "                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "                std=[1/0.229, 1/0.224, 1/0.225])\n",
        "\n",
        "            self.inv_seg_trans = transforms.Normalize(\n",
        "                mean=[-0.5/0.5],\n",
        "                std=[1/0.5])\n",
        "\n",
        "            log_path = os.path.join('.', 'log', '%s' % id)\n",
        "            self.logger = SummaryWriter(log_path)\n",
        "\n",
        "        # repo = git.Repo(\".\")\n",
        "        # self.log_string('git', str(repo.active_branch) + ' ' + str(repo.head.commit.hexsha))\n",
        "\n",
        "    def log_scalar(self, tag, x, step):\n",
        "        if self.no_log:\n",
        "            warnings.warn('Logging has been disabled.')\n",
        "            return\n",
        "        self.logger.add_scalar(tag, x, step)\n",
        "\n",
        "    def log_metrics(self, l1_tag, l2_tag, val, step, f=None):\n",
        "        tag = l1_tag + '/' + l2_tag\n",
        "        text = 'It {:8d} [{:5s}] [{:19s}]: {:s}'.format(step, l1_tag.upper(), l2_tag, fix_width_trunc(val))\n",
        "        print(text)\n",
        "        if f is not None:\n",
        "            f.write(text + '\\n')\n",
        "            f.flush()\n",
        "        self.log_scalar(tag, val, step)\n",
        "\n",
        "    def log_im(self, tag, x, step):\n",
        "        if self.no_log:\n",
        "            warnings.warn('Logging has been disabled.')\n",
        "            return\n",
        "        x = detach_to_cpu(x)\n",
        "        x = self.inv_im_trans(x)\n",
        "        x = tensor_to_numpy(x)\n",
        "        self.logger.add_image(tag, x, step)\n",
        "\n",
        "    def log_cv2(self, tag, x, step):\n",
        "        if self.no_log:\n",
        "            warnings.warn('Logging has been disabled.')\n",
        "            return\n",
        "        x = x.transpose((2, 0, 1))\n",
        "        self.logger.add_image(tag, x, step)\n",
        "\n",
        "    def log_seg(self, tag, x, step):\n",
        "        if self.no_log:\n",
        "            warnings.warn('Logging has been disabled.')\n",
        "            return\n",
        "        x = detach_to_cpu(x)\n",
        "        x = self.inv_seg_trans(x)\n",
        "        x = tensor_to_numpy(x)\n",
        "        self.logger.add_image(tag, x, step)\n",
        "\n",
        "    def log_gray(self, tag, x, step):\n",
        "        if self.no_log:\n",
        "            warnings.warn('Logging has been disabled.')\n",
        "            return\n",
        "        x = detach_to_cpu(x)\n",
        "        x = tensor_to_numpy(x)\n",
        "        self.logger.add_image(tag, x, step)\n",
        "\n",
        "    def log_string(self, tag, x):\n",
        "        print(tag, x)\n",
        "        if self.no_log:\n",
        "            warnings.warn('Logging has been disabled.')\n",
        "            return\n",
        "        self.logger.add_text(tag, x)\n",
        "\n",
        "    def log_total(self, tag, im, gt, seg, pred, step):\n",
        "        \n",
        "        if self.no_log:\n",
        "            warnings.warn('Logging has been disabled.')\n",
        "            return\n",
        "        \n",
        "        row_cnt = min(10, im.shape[0])\n",
        "        w = im.shape[2]\n",
        "        h = im.shape[3]\n",
        "        \n",
        "        output_image = np.zeros([3, w*row_cnt, h*5], dtype=np.uint8)\n",
        "        \n",
        "        for i in range(row_cnt):\n",
        "            im_ = tensor_to_numpy(self.inv_im_trans(detach_to_cpu(im[i])))\n",
        "            gt_ = tensor_to_numpy(detach_to_cpu(gt[i]))\n",
        "            seg_ = tensor_to_numpy(self.inv_seg_trans(detach_to_cpu(seg[i])))\n",
        "            pred_ = tensor_to_numpy(detach_to_cpu(pred[i]))\n",
        "            \n",
        "            output_image[:, i * w : (i+1) * w, 0 : h] = im_\n",
        "            output_image[:, i * w : (i+1) * w, h : 2*h] = gt_\n",
        "            output_image[:, i * w : (i+1) * w, 2*h : 3*h] = seg_\n",
        "            output_image[:, i * w : (i+1) * w, 3*h : 4*h] = pred_\n",
        "            output_image[:, i * w : (i+1) * w, 4*h : 5*h] = im_*0.5 + 0.5 * (im_ * (1-(pred_/255)) + (pred_/255) * (np.array([255,0,0],dtype=np.uint8).reshape([1,3,1,1])))\n",
        "            \n",
        "        self.logger.add_image(tag, output_image, step)"
      ],
      "metadata": {
        "id": "c0MD9CIDORrV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class ModelSaver:\n",
        "    def __init__(self, id):\n",
        "\n",
        "        if id is None:\n",
        "            self.no_log = True\n",
        "            print('Saving has been disbaled.')\n",
        "        else:\n",
        "            self.no_log = False\n",
        "\n",
        "            self.save_path = os.path.join('.', 'weights', '%s' % id )\n",
        "\n",
        "    def save_model(self, model, step):\n",
        "        if self.no_log:\n",
        "            print('Saving has been disabled.')\n",
        "            return\n",
        "\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "        model_path = os.path.join(self.save_path, 'model_%s' % step)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print('Model saved to %s.' % model_path)"
      ],
      "metadata": {
        "id": "q4mW6MnmOV-v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import ArgumentParser\n",
        "\n",
        "class HyperParameters():\n",
        "    def parse(self, unknown_arg_ok=False):\n",
        "        parser = ArgumentParser()\n",
        "        parser = {}\n",
        "        parser['iterations'] = 8000\n",
        "        parser['batch_size'] = 12\n",
        "        parser['lr'] = 2.25e-4\n",
        "        parser['steps'] = [22500, 37500]\n",
        "        parser['gamma'] = 0.1\n",
        "        parser['weight_decay'] = 1e-4\n",
        "        #parser['load'] = # path to pretrained model\n",
        "        parser['ce_weight'] = 0.0\n",
        "        parser['l1_weight'] = 1.0\n",
        "        parser['l2_weight'] = 1.0\n",
        "        parser['grad_weight'] = 5.0\n",
        "        parser['id'] = 'first'\n",
        "\n",
        "        return parser\n",
        "\n",
        "\n",
        "        # Generic learning parameters\n",
        "        #parser.add_argument('-i', '--iterations', help='Number of training iterations', default=4.5e4, type=int)\n",
        "        #print(\"added first argument\")\n",
        "        #parser.add_argument('-b', '--batch_size', help='Batch size', default=12, type=int)\n",
        "        #parser.add_argument('--lr', help='Initial learning rate', default=2.25e-4, type=float)\n",
        "        #parser.add_argument('--steps', help='Iteration at which learning rate is decayed by gamma', default=[22500, 37500], type=int, nargs='*')\n",
        "        #parser.add_argument('--gamma', help='Gamma used in learning rate decay', default=0.1, type=float)\n",
        "        #parser.add_argument('--weight_decay', help='Weight decay', default=1e-4, type=float)\n",
        "\n",
        "        # same decay applied to discriminator\n",
        "        #parser.add_argument('--load', help='Path to pretrained model if available')\n",
        "\n",
        "        #parser.add_argument('--ce_weight', help='Weight of the CE loss', default=0.0, type=float)\n",
        "        #parser.add_argument('--l1_weight', help='Weight of the L1 loss', default=1.0, type=float)\n",
        "        #parser.add_argument('--l2_weight', help='Weight of the L2 loss', default=1.0, type=float)\n",
        "        #parser.add_argument('--grad_weight', help='Weight of the gradient loss', default=5.0, type=float)\n",
        "\n",
        "        # Logging information, this one is positional and mandatory\n",
        "        #parser.add_argument('id', help='Experiment UNIQUE id, use NULL to disable logging to tensorboard')\n",
        "\n",
        "        #print(\"added all arguments\")\n",
        "\n",
        "        #if unknown_arg_ok:\n",
        "         #   args, _ = parser.parse_known_args()\n",
        "          #  self.args = vars(args)\n",
        "        #else:\n",
        "        #    print(\"unknown arg\")\n",
        "        #    pa = parser.parse_args(args=[])\n",
        "         #   print(\"passed first\")\n",
        "         #   self.args = vars(pa)\n",
        "         #   print(\"passed unknown args\")\n",
        "\n",
        "    #def __getitem__(self, key):\n",
        "     #   return self.args[key]\n",
        "\n",
        "    #def __str__(self):\n",
        "     #   return str(self.args)"
      ],
      "metadata": {
        "id": "EK2dXV1qOX3Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Integrate numerical values for some iterations\n",
        "Typically used for loss computation\n",
        "Just call finalize and create a new Integrator when you want to display \n",
        "\"\"\"\n",
        "class Integrator:\n",
        "    def __init__(self, logger):\n",
        "        self.values = {}\n",
        "        self.counts = {}\n",
        "        self.hooks  = [] # List is used here to maintain insertion order\n",
        "\n",
        "        self.logger = logger\n",
        "\n",
        "    def add_tensor(self, key, tensor):\n",
        "        if key not in self.values:\n",
        "            self.counts[key] = 1\n",
        "            if type(tensor) == float or type(tensor) == int:\n",
        "                self.values[key] = tensor\n",
        "            else:\n",
        "                self.values[key] = tensor.mean().item()\n",
        "        else:\n",
        "            self.counts[key] += 1\n",
        "            if type(tensor) == float or type(tensor) == int:\n",
        "                self.values[key] += tensor\n",
        "            else:\n",
        "                self.values[key] += tensor.mean().item()\n",
        "\n",
        "    def add_dict(self, tensor_dict):\n",
        "        for k, v in tensor_dict.items():\n",
        "            self.add_tensor(k, v)\n",
        "\n",
        "    def add_hook(self, hook):\n",
        "        \"\"\"\n",
        "        Adds a custom hook, i.e. compute new metrics using values in the dict\n",
        "        The hook takes the dict as argument, and returns a (k, v) tuple\n",
        "        \"\"\"\n",
        "        if type(hook) == list:\n",
        "            self.hooks.extend(hook)\n",
        "        else:\n",
        "            self.hooks.append(hook)\n",
        "\n",
        "    def reset_except_hooks(self):\n",
        "        self.values = {}\n",
        "        self.counts = {}\n",
        "\n",
        "    # Average and output the metrics\n",
        "    def finalize(self, prefix, iter, f=None):\n",
        "\n",
        "        for hook in self.hooks:\n",
        "            k, v = hook(self.values)\n",
        "            self.add_tensor(k, v)\n",
        "\n",
        "        for k, v in self.values.items():\n",
        "            avg = v / self.counts[k]\n",
        "\n",
        "            self.logger.log_metrics(prefix, k, avg, iter, f)"
      ],
      "metadata": {
        "id": "4E6qu0rxOZr7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def compute_tensor_iu(seg, gt):\n",
        "\n",
        "    #seg = seg.squeeze(1)\n",
        "    #gt = gt.squeeze(1)\n",
        "    \n",
        "    intersection = (seg & gt).float().sum()\n",
        "    union = (seg | gt).float().sum()\n",
        "\n",
        "    return intersection, union\n",
        "\n",
        "def compute_tensor_iou(seg, gt):\n",
        "    #seg = seg.squeeze(1)\n",
        "    #gt = gt.squeeze(1)\n",
        "    \n",
        "    intersection = (seg & gt).float().sum((1, 2))\n",
        "    union = (seg | gt).float().sum((1, 2))\n",
        "    \n",
        "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "    \n",
        "    return iou \n",
        "\n",
        "def resize_min_side(im, size, method):\n",
        "    h, w = im.shape[-2:]\n",
        "    \n",
        "    min_side = min(h, w)\n",
        "    ratio = size / min_side\n",
        "    if method == 'bilinear':\n",
        "        return F.interpolate(im, scale_factor=ratio, mode=method, align_corners=False)\n",
        "    else:\n",
        "        return F.interpolate(im, scale_factor=ratio, mode=method)\n",
        "\n",
        "def resize_max_side(im, size, method):\n",
        "    h, w = im.shape[-2:]\n",
        "    max_side = max(h, w)\n",
        "    ratio = size / max_side\n",
        "    if method in ['bilinear', 'bicubic']:\n",
        "        return F.interpolate(im, scale_factor=ratio, mode=method, align_corners=False)\n",
        "    else:\n",
        "        return F.interpolate(im, scale_factor=ratio, mode=method)"
      ],
      "metadata": {
        "id": "8Ge_p3foObsj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "#from util.util import compute_tensor_iu\n",
        "\n",
        "def get_new_iou_hook(values, size):\n",
        "    return 'iou/new_iou_%s'%size, values['iou/new_i_%s'%size]/values['iou/new_u_%s'%size]\n",
        "\n",
        "def get_orig_iou_hook(values):\n",
        "    return 'iou/orig_iou', values['iou/orig_i']/values['iou/orig_u']\n",
        "\n",
        "def get_iou_gain(values, size):\n",
        "    return 'iou/iou_gain_%s'%size, values['iou/new_iou_%s'%size] - values['iou/orig_iou']\n",
        "\n",
        "iou_hooks_to_be_used = [\n",
        "        get_orig_iou_hook,\n",
        "        lambda x: get_new_iou_hook(x, '224'), lambda x: get_iou_gain(x, '224'),\n",
        "    ]\n",
        "\n",
        "iou_hooks_final_only = [\n",
        "    get_orig_iou_hook,\n",
        "    lambda x: get_new_iou_hook(x, '224'), lambda x: get_iou_gain(x, '224'),\n",
        "]\n",
        "\n",
        "# Compute common loss and metric for generator only\n",
        "def compute_loss_and_metrics(images, para, detailed=True, need_loss=True, has_lower_res=True):\n",
        "\n",
        "    \"\"\"\n",
        "    This part compute loss and metrics for the generator\n",
        "    \"\"\"\n",
        "\n",
        "    loss_and_metrics = {}\n",
        "\n",
        "    gt = images['gt']\n",
        "    seg = images['seg']\n",
        "\n",
        "\n",
        "    #seg = seg.argmax(1)\n",
        "\n",
        "    pred_224 = images['pred_224']\n",
        "\n",
        "    if need_loss:\n",
        "        # Loss weights\n",
        "        ce_weights = para['ce_weight']\n",
        "        l1_weights = para['l1_weight']\n",
        "        l2_weights = para['l2_weight']\n",
        "\n",
        "        # temp holder for losses at different scale\n",
        "        ce_loss = 0 \n",
        "        l1_loss = 0 \n",
        "        l2_loss = 0 \n",
        "        loss = 0 \n",
        "\n",
        "        ce_loss = F.binary_cross_entropy_with_logits(images['out_224'], (gt>0.5).float())\n",
        "        l1_loss = F.l1_loss(pred_224, gt)\n",
        "        l2_loss = F.mse_loss(pred_224, gt)\n",
        "\n",
        "        loss_and_metrics['grad_loss'] = F.l1_loss(images['gt_sobel'], images['pred_sobel'])\n",
        "\n",
        "        # Weighted loss for different levels\n",
        "        loss = ce_loss * ce_weights + l1_loss * l1_weights + l2_loss * l2_weights\n",
        "        \n",
        "        loss += loss_and_metrics['grad_loss'] * para['grad_weight']\n",
        "    \n",
        "    #metric = SegmentationMetric(20)\n",
        "\n",
        "\n",
        "    #metric.update(seg, gt)\n",
        "\n",
        "    #pixAcc, mIoU = metric.get()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \"\"\"\n",
        "    Compute IOU stats\n",
        "    \"\"\"\n",
        "    #print(torch.nonzero(seg))\n",
        "    orig_total_i, orig_total_u = compute_tensor_iu(seg>0.5, gt>0.5)\n",
        "    loss_and_metrics['iou/orig_i'] = orig_total_i\n",
        "    loss_and_metrics['iou/orig_u'] = orig_total_u\n",
        "\n",
        "    new_total_i, new_total_u = compute_tensor_iu(pred_224>0.5, gt>0.5)\n",
        "    loss_and_metrics['iou/new_i_224'] = new_total_i\n",
        "    loss_and_metrics['iou/new_u_224'] = new_total_u\n",
        "    #loss_and_metrics['mIoU'] = mIoU\n",
        "        \n",
        "    \"\"\"\n",
        "    All done.\n",
        "    Now gather everything in a dict for logging\n",
        "    \"\"\"\n",
        "\n",
        "    if need_loss:\n",
        "        loss_and_metrics['total_loss'] = 0\n",
        "        loss_and_metrics['ce_loss'] = ce_loss\n",
        "        loss_and_metrics['l1_loss'] = l1_loss\n",
        "        loss_and_metrics['l2_loss'] = l2_loss\n",
        "        loss_and_metrics['loss'] = loss\n",
        "\n",
        "        loss_and_metrics['total_loss'] += loss\n",
        "\n",
        "    return loss_and_metrics"
      ],
      "metadata": {
        "id": "Bxkz-nsAOfMM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "inv_im_trans = transforms.Normalize(\n",
        "                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "                std=[1/0.229, 1/0.224, 1/0.225])\n",
        "\n",
        "inv_seg_trans = transforms.Normalize(\n",
        "    mean=[-0.5/0.5],\n",
        "    std=[1/0.5])\n",
        "\n",
        "def tensor_to_numpy(image):\n",
        "    image_np = (image.numpy() * 255).astype('uint8')\n",
        "    return image_np\n",
        "\n",
        "def tensor_to_np_float(image):\n",
        "    image_np = image.numpy().astype('float32')\n",
        "    return image_np\n",
        "\n",
        "def detach_to_cpu(x):\n",
        "    return x.detach().cpu()\n",
        "\n",
        "def transpose_np(x):\n",
        "    return np.transpose(x, [1,2,0])\n",
        "\n",
        "def tensor_to_gray_im(x):\n",
        "    x = detach_to_cpu(x)\n",
        "    x = tensor_to_numpy(x)\n",
        "    x = transpose_np(x)\n",
        "    return x\n",
        "\n",
        "def tensor_to_seg(x):\n",
        "    x = detach_to_cpu(x)\n",
        "    x = inv_seg_trans(x)\n",
        "    x = tensor_to_numpy(x)\n",
        "    x = transpose_np(x)\n",
        "    return x\n",
        "\n",
        "def tensor_to_im(x):\n",
        "    x = detach_to_cpu(x)\n",
        "    x = inv_im_trans(x)\n",
        "    x = tensor_to_numpy(x)\n",
        "    x = transpose_np(x)\n",
        "    return x\n",
        "\n",
        "# Predefined key <-> caption dict\n",
        "key_captions = {\n",
        "    'im': 'Image', \n",
        "    'gt': 'GT', \n",
        "    'seg': 'Input', \n",
        "    'error_map': 'Error map',\n",
        "}\n",
        "for k in ['28', '56', '224']:\n",
        "    key_captions['pred_' + k] = 'Ours-%sx%s' % (k, k)\n",
        "    key_captions['pred_' + k + '_overlay'] = '%sx%s' % (k, k)\n",
        "\n",
        "\"\"\"\n",
        "Return an image array with captions\n",
        "keys in dictionary will be used as caption if not provided\n",
        "values should contain lists of cv2 images\n",
        "\"\"\"\n",
        "def get_image_array(images, grid_shape, captions={}):\n",
        "    w, h = grid_shape\n",
        "    cate_counts = len(images)\n",
        "    rows_counts = len(next(iter(images.values())))\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "    output_image = np.zeros([h*(rows_counts+1), w*cate_counts, 3], dtype=np.uint8)\n",
        "    col_cnt = 0\n",
        "    for k, v in images.items():\n",
        "\n",
        "        # Default as key value itself\n",
        "        caption = captions.get(k, k)\n",
        "\n",
        "        # Handles new line character\n",
        "        y0, dy = h-10-len(caption.split('\\n'))*40, 40\n",
        "        for i, line in enumerate(caption.split('\\n')):\n",
        "            y = y0 + i*dy\n",
        "            cv2.putText(output_image, line, (col_cnt*w, y),\n",
        "                     font, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
        "\n",
        "        # Put images\n",
        "        for row_cnt, img in enumerate(v):\n",
        "            im_shape = img.shape\n",
        "            if len(im_shape) == 2:\n",
        "                img = img[..., np.newaxis]\n",
        "\n",
        "            img = (img * 255).astype('uint8')\n",
        "\n",
        "            output_image[(row_cnt+1)*h:(row_cnt+2)*h,\n",
        "                         col_cnt*w:(col_cnt+1)*w, :] = img\n",
        "            \n",
        "        col_cnt += 1\n",
        "\n",
        "    return output_image\n",
        "\n",
        "\"\"\"\n",
        "Create an image array, transform each image separately as needed\n",
        "Will only put images in req_keys\n",
        "\"\"\"\n",
        "def pool_images(images, req_keys, row_cnt=10):\n",
        "    req_images = {}\n",
        "\n",
        "    def base_transform(im):\n",
        "        im = tensor_to_np_float(im)\n",
        "        im = im.transpose((1, 2, 0))\n",
        "\n",
        "        # Resize\n",
        "        if im.shape[1] != 224:\n",
        "            im = cv2.resize(im, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        if len(im.shape) == 2:\n",
        "            im = im[..., np.newaxis]\n",
        "\n",
        "        return im\n",
        "\n",
        "    second_pass_keys = []\n",
        "    for k in req_keys:\n",
        "\n",
        "        if 'overlay' in k: \n",
        "            # Run overlay in the second pass, skip for now\n",
        "            second_pass_keys.append(k)\n",
        "\n",
        "            # Make sure the base key information is transformed\n",
        "            base_key = k.replace('_overlay', '')\n",
        "            if base_key in req_keys:\n",
        "                continue\n",
        "            else:\n",
        "                k = base_key\n",
        "\n",
        "        req_images[k] = []\n",
        "\n",
        "        images[k] = detach_to_cpu(images[k])\n",
        "        for i in range(min(row_cnt, len(images[k]))):\n",
        "\n",
        "            im = images[k][i]\n",
        "\n",
        "            # Handles inverse transform\n",
        "            if k in ['im']:\n",
        "                im = inv_im_trans(images[k][i])\n",
        "            elif k in ['seg']:\n",
        "                im = inv_seg_trans(images[k][i])\n",
        "\n",
        "            # Now we are all numpy array\n",
        "            im = base_transform(im)\n",
        "\n",
        "            req_images[k].append(im)\n",
        "\n",
        "    # Handle overlay images in the second pass\n",
        "    for k in second_pass_keys:\n",
        "        req_images[k] = []\n",
        "        base_key = k.replace('_overlay', '')\n",
        "        for i in range(min(row_cnt, len(images[base_key]))):\n",
        "\n",
        "            # If overlay\n",
        "            im = req_images[base_key][i]\n",
        "            raw = req_images['im'][i]\n",
        "\n",
        "            im = im.clip(0, 1)\n",
        "\n",
        "            # Just red overlay\n",
        "            im = (raw*0.5 + 0.5 * (raw * (1-im) \n",
        "                    + im * (np.array([1,0,0],dtype=np.float32)\n",
        "                    .reshape([1,1,3]))))\n",
        "            \n",
        "            req_images[k].append(im)\n",
        "    \n",
        "    # Remove all temp items\n",
        "    output_images = {}\n",
        "    for k in req_keys:\n",
        "        output_images[k] = req_images[k]\n",
        "\n",
        "    return get_image_array(output_images, (224, 224), key_captions)\n",
        "\n",
        "# Return cv2 image, directly usable for saving\n",
        "def vis_prediction(images):\n",
        "\n",
        "    keys = ['im', 'seg', 'gt', 'pred_224', 'pred_224_overlay'] # 'pred_28', 'pred_28_2', 'pred_56', 'pred_28_3', 'pred_56_2', \n",
        "\n",
        "    return pool_images(images, keys)"
      ],
      "metadata": {
        "id": "e-tDy5e6OjfN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def get_random_structure(size):\n",
        "    # The provided model is trained with \n",
        "    #   choice = np.random.randint(4)\n",
        "    # instead, which is a bug that we fixed here\n",
        "    choice = np.random.randint(1, 5)\n",
        "\n",
        "    if choice == 1:\n",
        "        return cv2.getStructuringElement(cv2.MORPH_RECT, (size, size))\n",
        "    elif choice == 2:\n",
        "        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size))\n",
        "    elif choice == 3:\n",
        "        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size//2))\n",
        "    elif choice == 4:\n",
        "        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size//2, size))\n",
        "\n",
        "def random_dilate(seg, min=3, max=10):\n",
        "    size = np.random.randint(min, max)\n",
        "    kernel = get_random_structure(size)\n",
        "    seg = cv2.dilate(seg,kernel,iterations = 1)\n",
        "    return seg\n",
        "\n",
        "def random_erode(seg, min=3, max=10):\n",
        "    size = np.random.randint(min, max)\n",
        "    kernel = get_random_structure(size)\n",
        "    seg = cv2.erode(seg,kernel,iterations = 1)\n",
        "    return seg\n",
        "\n",
        "def compute_iou(seg, gt):\n",
        "    intersection = seg*gt\n",
        "    union = seg+gt\n",
        "    return (np.count_nonzero(intersection) + 1e-6) / (np.count_nonzero(union) + 1e-6)\n",
        "\n",
        "def perturb_seg(gt, iou_target=0.6):\n",
        "    h, w = gt.shape\n",
        "    seg = gt.copy()\n",
        "\n",
        "    _, seg = cv2.threshold(seg, 127, 255, 0)\n",
        "\n",
        "    # Rare case\n",
        "    if h <= 2 or w <= 2:\n",
        "        print('GT too small, returning original')\n",
        "        return seg\n",
        "\n",
        "    # Do a bunch of random operations\n",
        "    for _ in range(250):\n",
        "        for _ in range(4):\n",
        "            lx, ly = np.random.randint(w), np.random.randint(h)\n",
        "            lw, lh = np.random.randint(lx+1,w+1), np.random.randint(ly+1,h+1)\n",
        "\n",
        "            # Randomly set one pixel to 1/0. With the following dilate/erode, we can create holes/external regions\n",
        "            if np.random.rand() < 0.25:\n",
        "                cx = int((lx + lw) / 2)\n",
        "                cy = int((ly + lh) / 2)\n",
        "                seg[cy, cx] = np.random.randint(2) * 255\n",
        "\n",
        "            if np.random.rand() < 0.5:\n",
        "                seg[ly:lh, lx:lw] = random_dilate(seg[ly:lh, lx:lw])\n",
        "            else:\n",
        "                seg[ly:lh, lx:lw] = random_erode(seg[ly:lh, lx:lw])\n",
        "\n",
        "        if compute_iou(seg, gt) < iou_target:\n",
        "            break\n",
        "\n",
        "    return seg"
      ],
      "metadata": {
        "id": "yIBZyH1hP039"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "##try:\n",
        "   # from util.de_transform import perturb_seg\n",
        "#except:\n",
        "  #  from de_transform import perturb_seg\n",
        "\n",
        "\n",
        "def modify_boundary(image, regional_sample_rate=0.1, sample_rate=0.1, move_rate=0.0, iou_target = 0.8):\n",
        "    # modifies boundary of the given mask.\n",
        "    # remove consecutive vertice of the boundary by regional sample rate\n",
        "    # ->\n",
        "    # remove any vertice by sample rate\n",
        "    # ->\n",
        "    # move vertice by distance between vertice and center of the mask by move rate. \n",
        "    # input: np array of size [H,W] image\n",
        "    # output: same shape as input\n",
        "    \n",
        "    # get boundaries\n",
        "    if int(cv2.__version__[0]) >= 4:\n",
        "        contours, _ = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "    else:\n",
        "        _, contours, _ = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "    #only modified contours is needed actually. \n",
        "    sampled_contours = []   \n",
        "    modified_contours = [] \n",
        "\n",
        "    for contour in contours:\n",
        "        if contour.shape[0] < 10:\n",
        "            continue\n",
        "        M = cv2.moments(contour)\n",
        "\n",
        "        #remove region of contour\n",
        "        number_of_vertices = contour.shape[0]\n",
        "        number_of_removes = int(number_of_vertices * regional_sample_rate)\n",
        "        \n",
        "        idx_dist = []\n",
        "        for i in range(number_of_vertices-number_of_removes):\n",
        "            idx_dist.append([i, np.sum((contour[i] - contour[i+number_of_removes])**2)])\n",
        "            \n",
        "        idx_dist = sorted(idx_dist, key=lambda x:x[1])\n",
        "        \n",
        "        remove_start = random.choice(idx_dist[:math.ceil(0.1*len(idx_dist))])[0]\n",
        "        \n",
        "       #remove_start = random.randrange(0, number_of_vertices-number_of_removes, 1)\n",
        "        new_contour = np.concatenate([contour[:remove_start], contour[remove_start+number_of_removes:]], axis=0)\n",
        "        contour = new_contour\n",
        "        \n",
        "\n",
        "        #sample contours\n",
        "        number_of_vertices = contour.shape[0]\n",
        "        indices = random.sample(range(number_of_vertices), int(number_of_vertices * sample_rate))\n",
        "        indices.sort()\n",
        "        sampled_contour = contour[indices]\n",
        "        sampled_contours.append(sampled_contour)\n",
        "\n",
        "        modified_contour = np.copy(sampled_contour)\n",
        "        if (M['m00'] != 0):\n",
        "            center = round(M['m10'] / M['m00']), round(M['m01'] / M['m00'])\n",
        "\n",
        "            #modify contours\n",
        "            for idx, coor in enumerate(modified_contour):\n",
        "\n",
        "                change = np.random.normal(0,move_rate) # 0.1 means change position of vertex to 10 percent farther from center\n",
        "                x,y = coor[0]\n",
        "                new_x = x + (x-center[0]) * change\n",
        "                new_y = y + (y-center[1]) * change\n",
        "\n",
        "                modified_contour[idx] = [new_x,new_y]\n",
        "        modified_contours.append(modified_contour)\n",
        "        \n",
        "\n",
        "    #draw boundary\n",
        "    gt = np.copy(image)\n",
        "    image = np.zeros_like(image)\n",
        "\n",
        "    modified_contours = [cont for cont in modified_contours if len(cont) > 0]\n",
        "    if len(modified_contours) == 0:\n",
        "        image = gt.copy()\n",
        "    else:\n",
        "        image = cv2.drawContours(image, modified_contours, -1, (255, 0, 0), -1)\n",
        "\n",
        "    image = perturb_seg(image, iou_target)\n",
        "    \n",
        "    return image"
      ],
      "metadata": {
        "id": "vb83ZJ1OP1kK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_seg_as_input(seg_image):\n",
        "\n",
        "  I = np.eye(20)\n",
        "\n",
        "  returned_image = I[seg_image]\n",
        "\n",
        "  returned_image = np.transpose(returned_image, (2, 0, 1))\n",
        "\n",
        "  return returned_image"
      ],
      "metadata": {
        "id": "tswYoVoP3_9g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def expand_classes(gt, num_classes):\n",
        "  I = np.eye(num_classes)\n",
        "  returned_image = I[gt.numpy()]\n",
        "  returned_image = torch.from_numpy(returned_image)\n",
        "  returned_image = returned_image.squeeze(0)\n",
        "  gt = np.transpose(returned_image, (2, 0, 1))\n",
        "  gt = torch.tensor(gt, dtype=torch.float)\n",
        "  return gt"
      ],
      "metadata": {
        "id": "Dgmz1HHg4SU6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import path\n",
        "import warnings\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "#from dataset.reseed import reseed\n",
        "#import util.boundary_modification as boundary_modification\n",
        "\n",
        "import torch\n",
        "\n",
        "seg_normalization = transforms.Normalize(\n",
        "                mean=[0.5],\n",
        "                std=[0.5]\n",
        "            )\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def reseed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def make_coord(shape, ranges=None, flatten=True):\n",
        "    \"\"\" Make coordinates at grid centers.\n",
        "    \"\"\"\n",
        "    coord_seqs = []\n",
        "    for i, n in enumerate(shape):\n",
        "        if ranges is None:\n",
        "            v0, v1 = -1, 1\n",
        "        else:\n",
        "            v0, v1 = ranges[i]\n",
        "        r = (v1 - v0) / (2 * n)\n",
        "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
        "        coord_seqs.append(seq)\n",
        "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
        "    if flatten:\n",
        "        ret = ret.view(-1, ret.shape[-1])\n",
        "    return ret\n",
        "\n",
        "\n",
        "def to_pixel_samples(img):\n",
        "    \"\"\" Convert the image to coord-RGB pairs.\n",
        "        img: Tensor, (3, H, W)\n",
        "    \"\"\"\n",
        "    coord = make_coord_constant2\n",
        "    #coord = make_coord(img.shape[-2:])\n",
        "    rgb = img.view(1, -1).permute(1, 0)\n",
        "    return coord, rgb\n",
        "\n",
        "\n",
        "def resize_fn(img, size):\n",
        "    return transforms.ToTensor()(\n",
        "        transforms.Resize(size, Image.BICUBIC)(\n",
        "            transforms.ToPILImage()(img)))\n",
        "\n",
        "\n",
        "class OnlineTransformDataset_Second(Dataset):\n",
        "    \"\"\"\n",
        "    Method 0 - FSS style (class/1.jpg class/1.png)\n",
        "    Method 1 - Others style (XXX.jpg XXX.png)\n",
        "    \"\"\"\n",
        "    def __init__(self, root, need_name=False, method=0, perturb=True, test = False):\n",
        "        self.root = root\n",
        "        self.need_name = need_name\n",
        "        self.method = method\n",
        "\n",
        "        #self.im_list = []\n",
        "\n",
        "\n",
        "        # code goes here\n",
        "\n",
        "        self.im_list = []\n",
        "        self.mask_list = []\n",
        "        self.seg_list = []\n",
        "\n",
        "        self.flag = True\n",
        "\n",
        "\n",
        "        img_folder = \"/content/drive/MyDrive/multiclass-seg/cityscapes/leftImg8bit/train\"\n",
        "        mask_folder = \"/content/drive/MyDrive/multiclass-seg/cityscapes/gtFine/train\"\n",
        "        seg_folder = \"/content/drive/MyDrive/multiclass-seg/cityscapes/segs\"\n",
        "\n",
        "\n",
        "        if test == True:\n",
        "          train_image_path = \"/content/drive/MyDrive/multiclass-seg/cityscapes/leftImg8bit/train/hamburg/hamburg_000000_000042_leftImg8bit.png\"\n",
        "          train_seg_path = \"/content/drive/MyDrive/multiclass-seg/cityscapes/segs/hamburg_000000_000042_leftImg8bit.png\"\n",
        "          train_gt = \"/content/drive/MyDrive/multiclass-seg/cityscapes/gtFine/train/hamburg/hamburg_000000_000042_gtFine_color.png\"\n",
        "\n",
        "          self.im_list.append(train_image_path)\n",
        "          self.mask_list.append(train_seg_path)\n",
        "          self.seg_list.append(train_gt)\n",
        "\n",
        "        else:\n",
        "\n",
        "\n",
        "          #####\n",
        "\n",
        "          for root, _, files in os.walk(img_folder):\n",
        "              for filename in files:\n",
        "                  if filename.endswith('.png'):\n",
        "\n",
        "                      imgpath = os.path.join(root, filename)\n",
        "                      foldername = os.path.basename(os.path.dirname(imgpath))\n",
        "                      maskname = filename.replace('leftImg8bit', 'gtFine_labelTrainIds')\n",
        "                      maskpath = os.path.join(mask_folder, foldername, maskname)\n",
        "                      segPath = os.path.join(seg_folder, filename)\n",
        "                      if os.path.isfile(imgpath) and os.path.isfile(maskpath) and os.path.isfile(segPath):\n",
        "                          self.im_list.append(imgpath)\n",
        "                          self.mask_list.append(maskpath)\n",
        "                          self.seg_list.append(segPath)\n",
        "\n",
        "                      else:\n",
        "                          print('cannot find the mask or image:', imgpath)\n",
        "\n",
        "          #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#        if method == 0:\n",
        " #           # Get images\n",
        "  #          self.im_list = []\n",
        "   #         classes = os.listdir(self.root)\n",
        "    #        for c in classes:\n",
        "     #           imgs = os.listdir(path.join(root, c))\n",
        "      #          jpg_list = [im for im in imgs if 'jpg' in im[-3:].lower()]\n",
        "       #         unmatched = any([im.replace('.jpg', '.png') not in imgs for im in jpg_list])\n",
        "\n",
        "        #        if unmatched:\n",
        "         #           print('Number of image/gt unmatch in class ', c)\n",
        "          #          print('The whole class is ignored', len(jpg_list))\n",
        "\n",
        "           #         warnings.warn('Dataset unmatch error')\n",
        "            #    else:\n",
        "             #       joint_list = [path.join(root, c, im) for im in jpg_list]\n",
        "              #      self.im_list.extend(joint_list)\n",
        "\n",
        "        #elif method == 1:\n",
        "         #   self.im_list = [path.join(self.root, im) for im in os.listdir(self.root) if '.jpg' in im]\n",
        "\n",
        "        print('%d images found' % len(self.im_list))\n",
        "\n",
        "        if perturb:\n",
        "            # Make up some transforms\n",
        "            self.bilinear_dual_transform = transforms.Compose([\n",
        "                transforms.RandomCrop((440, 880), pad_if_needed=True),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "            ])\n",
        "\n",
        "            self.bilinear_dual_transform_im = transforms.Compose([\n",
        "                transforms.RandomCrop((440, 880), pad_if_needed=True),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "            ])\n",
        "\n",
        "            self.im_transform = transforms.Compose([\n",
        "                transforms.ColorJitter(0.2, 0.05, 0.05, 0),\n",
        "                transforms.RandomGrayscale(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ])\n",
        "        else:\n",
        "            # Make up some transforms\n",
        "            self.bilinear_dual_transform = transforms.Compose([\n",
        "                transforms.Resize((220, 440), interpolation=Image.NEAREST), \n",
        "                transforms.CenterCrop((220, 440)),\n",
        "            ])\n",
        "\n",
        "            self.bilinear_dual_transform_im = transforms.Compose([\n",
        "                transforms.Resize((220, 440), interpolation=Image.BILINEAR), \n",
        "                transforms.CenterCrop((220, 440)),\n",
        "            ])\n",
        "\n",
        "            self.im_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ])\n",
        "\n",
        "        self.gt_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.seg_transform1 = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        self.seg_transform2 = transforms.Compose([\n",
        "            seg_normalization\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        #print(self.im_list[idx] + \", \" + self.mask_list[idx] + \", \" + self.seg_list[idx])\n",
        "\n",
        "\n",
        "        im = Image.open(self.im_list[idx]).convert('RGB')  # im = Image.open(self.im_list[idx]).convert('RGB')\n",
        "\n",
        "        #############################################################\n",
        "        #############################################################\n",
        "        #############################################################\n",
        "        #############################################################\n",
        "        #############################################################\n",
        "\n",
        "        #if self.method == 0:\n",
        "         #   gt = Image.open(self.im_list[idx][:-3]+'png').convert('L')\n",
        "        #else:\n",
        "         #   path = self.im_list[idx].replace('.jpg','.png')\n",
        "          #  gt = Image.open(path).convert('L')\n",
        "\n",
        "        #seed = np.random.randint(2147483647)\n",
        "        \n",
        "        #reseed(seed)\n",
        "        #im = self.bilinear_dual_transform_im(im)\n",
        "\n",
        "        #reseed(seed)\n",
        "        #gt = self.bilinear_dual_transform(gt)\n",
        "\n",
        "        #iou_max = 1.0\n",
        "        #iou_min = 0.8\n",
        "        #iou_target = np.random.rand()*(iou_max-iou_min) + iou_min\n",
        "        #seg = modify_boundary((np.array(gt)>0.5).astype('uint8')*255, iou_target=iou_target)\n",
        "\n",
        "\n",
        "        #############################################################\n",
        "        #############################################################\n",
        "        #############################################################\n",
        "        #############################################################\n",
        "        #############################################################\n",
        "\n",
        "        ############################\n",
        "\n",
        "        im = self.bilinear_dual_transform_im(im)\n",
        "\n",
        "        gt = self.mask_list[idx]\n",
        "        gt = Image.open(gt)\n",
        "        gt = self.bilinear_dual_transform(gt)\n",
        "\n",
        "        seg_image = self.seg_list[idx]\n",
        "        seg_image = Image.open(seg_image)\n",
        "        seg_image = self.bilinear_dual_transform(seg_image)\n",
        "\n",
        "        #seg_image = np.where(seg_image == 255, 0, seg_image)\n",
        "        #seg_image = get_seg_as_input(seg_image)\n",
        "\n",
        "        #seg_image *= 255\n",
        "\n",
        "        \n",
        "\n",
        "        ############################\n",
        "\n",
        "        gt = np.array(gt).astype('int32')\n",
        "\n",
        "        im = self.im_transform(im)\n",
        "        gt = self.seg_transform1(gt)\n",
        "        seg = self.seg_transform1(seg_image)\n",
        "\n",
        "        seg = np.where(seg_image == 255, 0, seg_image)\n",
        "        gt = np.where(gt == 255, 0, gt)\n",
        "\n",
        "        #print(gt[0, 200:400, 200:400])\n",
        "\n",
        "\n",
        "\n",
        "        seg = get_seg_as_input(seg)\n",
        "        seg = torch.from_numpy(seg)\n",
        "        seg = seg.float()\n",
        "\n",
        "        #seg = seg.reshape(20, 224, 224)\n",
        "        seg = self.seg_transform2(seg)\n",
        "\n",
        "\n",
        "\n",
        "        hr_coord, hr_rgb = to_pixel_samples(seg.contiguous())\n",
        "\n",
        "        cell = torch.ones_like(hr_coord)\n",
        "        cell[:, 0] *= 2 / seg.shape[-2] \n",
        "        cell[:, 1] *= 2 / seg.shape[-1]\n",
        "\n",
        "        #crop_lr = resize_fn(seg, seg.shape[-2]) # \n",
        "\n",
        "        im = im.float()\n",
        "        seg = seg.float()\n",
        "        gt = torch.from_numpy(gt)\n",
        "        gt = expand_classes(gt, 20)\n",
        "\n",
        "        if self.need_name:\n",
        "            return im, seg, gt, os.path.basename(self.im_list[idx][:-4])\n",
        "        else:\n",
        "            return im, seg, gt, {'coord': hr_coord, 'cell': cell, 'gt': hr_rgb}   # return im, seg, gt, {'inp': crop_lr, 'coord': hr_coord, 'cell': cell, 'gt': hr_rgb}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.im_list)\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    ecssd_dir = '/PathTo/data/ecssd'\n",
        "#    ecssd_dataset = OnlineTransformDataset(ecssd_dir, method=1, perturb=True)\n",
        "\n",
        "#    import pdb; pdb.set_trace()\n",
        "#    ecssd_dataset[0]"
      ],
      "metadata": {
        "id": "k28-Qjgt584P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import path\n",
        "import warnings\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "#from dataset.reseed import reseed\n",
        "#import util.boundary_modification as boundary_modification\n",
        "\n",
        "import torch\n",
        "\n",
        "seg_normalization = transforms.Normalize(\n",
        "                mean=[0.5],\n",
        "                std=[0.5]\n",
        "            )\n",
        "\n",
        "def make_coord(shape, ranges=None, flatten=True):\n",
        "    \"\"\" Make coordinates at grid centers.\n",
        "    \"\"\"\n",
        "    coord_seqs = []\n",
        "    for i, n in enumerate(shape):\n",
        "        if ranges is None:\n",
        "            v0, v1 = -1, 1\n",
        "        else:\n",
        "            v0, v1 = ranges[i]\n",
        "        r = (v1 - v0) / (2 * n)\n",
        "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
        "        coord_seqs.append(seq)\n",
        "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
        "    if flatten:\n",
        "        ret = ret.view(-1, ret.shape[-1])\n",
        "    return ret\n",
        "\n",
        "\n",
        "def to_pixel_samples(img):\n",
        "    \"\"\" Convert the image to coord-RGB pairs.\n",
        "        img: Tensor, (3, H, W)\n",
        "    \"\"\"\n",
        "    coord = make_coord(img.shape[-2:])\n",
        "    rgb = img.view(1, -1).permute(1, 0)\n",
        "    return coord, rgb\n",
        "\n",
        "\n",
        "def resize_fn(img, size):\n",
        "    return transforms.ToTensor()(\n",
        "        transforms.Resize(size, Image.BICUBIC)(\n",
        "            transforms.ToPILImage()(img)))\n",
        "\n",
        "\n",
        "class OnlineTransformDataset_crm(Dataset):\n",
        "    \"\"\"\n",
        "    Method 0 - FSS style (class/1.jpg class/1.png)\n",
        "    Method 1 - Others style (XXX.jpg XXX.png)\n",
        "    \"\"\"\n",
        "    def __init__(self, root, need_name=False, method=0, perturb=True):\n",
        "        self.root = root\n",
        "        self.need_name = need_name\n",
        "        self.method = method\n",
        "\n",
        "\n",
        "        if method == 0:\n",
        "            # Get images\n",
        "            self.im_list = []\n",
        "            classes = os.listdir(self.root)\n",
        "            for c in classes:\n",
        "                imgs = os.listdir(path.join(root, c))\n",
        "                jpg_list = [im for im in imgs if 'jpg' in im[-3:].lower()]\n",
        "                unmatched = any([im.replace('.jpg', '.png') not in imgs for im in jpg_list])\n",
        "\n",
        "                if unmatched:\n",
        "                    print('Number of image/gt unmatch in class ', c)\n",
        "                    print('The whole class is ignored', len(jpg_list))\n",
        "\n",
        "                    warnings.warn('Dataset unmatch error')\n",
        "                else:\n",
        "                    joint_list = [path.join(root, c, im) for im in jpg_list]\n",
        "                    self.im_list.extend(joint_list)\n",
        "\n",
        "        elif method == 1:\n",
        "            self.im_list = [path.join(self.root, im) for im in os.listdir(self.root) if '.jpg' in im]\n",
        "\n",
        "        print('%d images found' % len(self.im_list))\n",
        "\n",
        "        if perturb:\n",
        "            # Make up some transforms\n",
        "            self.bilinear_dual_transform = transforms.Compose([\n",
        "                transforms.RandomCrop((224, 224), pad_if_needed=True),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "            ])\n",
        "\n",
        "            self.bilinear_dual_transform_im = transforms.Compose([\n",
        "                transforms.RandomCrop((224, 224), pad_if_needed=True),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "            ])\n",
        "\n",
        "            self.im_transform = transforms.Compose([\n",
        "                transforms.ColorJitter(0.2, 0.05, 0.05, 0),\n",
        "                transforms.RandomGrayscale(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ])\n",
        "        else:\n",
        "            # Make up some transforms\n",
        "            self.bilinear_dual_transform = transforms.Compose([\n",
        "                transforms.Resize(224, interpolation=Image.NEAREST), \n",
        "                transforms.CenterCrop(224),\n",
        "            ])\n",
        "\n",
        "            self.bilinear_dual_transform_im = transforms.Compose([\n",
        "                transforms.Resize(224, interpolation=Image.BILINEAR), \n",
        "                transforms.CenterCrop(224),\n",
        "            ])\n",
        "\n",
        "            self.im_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ])\n",
        "\n",
        "        self.gt_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.seg_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            seg_normalization,\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        im = Image.open(self.im_list[idx]).convert('RGB')\n",
        "\n",
        "        if self.method == 0:\n",
        "            gt = Image.open(self.im_list[idx][:-3]+'png').convert('L')\n",
        "        else:\n",
        "            gt = Image.open(self.im_list[idx].replace('.jpg','.png')).convert('L')\n",
        "\n",
        "        seed = np.random.randint(2147483647)\n",
        "        \n",
        "        reseed(seed)\n",
        "        im = self.bilinear_dual_transform_im(im)\n",
        "\n",
        "        reseed(seed)\n",
        "        gt = self.bilinear_dual_transform(gt)\n",
        "\n",
        "        iou_max = 1.0\n",
        "        iou_min = 0.8\n",
        "        iou_target = np.random.rand()*(iou_max-iou_min) + iou_min\n",
        "        seg = modify_boundary((np.array(gt)>0.5).astype('uint8')*255, iou_target=iou_target)\n",
        "\n",
        "        temp = seg\n",
        "\n",
        "        im = self.im_transform(im)\n",
        "        gt = self.gt_transform(gt)\n",
        "        seg = self.seg_transform(seg)\n",
        "\n",
        "\n",
        "        hr_coord, hr_rgb = to_pixel_samples(seg.contiguous())\n",
        "\n",
        "        cell = torch.ones_like(hr_coord)\n",
        "        cell[:, 0] *= 2 / seg.shape[-2] \n",
        "        cell[:, 1] *= 2 / seg.shape[-1]\n",
        "\n",
        "        crop_lr = resize_fn(seg, seg.shape[-2]) # \n",
        "\n",
        "        if self.need_name:\n",
        "            return im, seg, gt, os.path.basename(self.im_list[idx][:-4])\n",
        "        else:\n",
        "            return im, seg, gt, {'inp': crop_lr, 'coord': hr_coord, 'cell': cell, 'gt': hr_rgb}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.im_list)"
      ],
      "metadata": {
        "id": "24zqSVLuP3qu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "#from models.network.crm import CRMNet\n",
        "#from models.sobel_op import SobelComputer\n",
        "\n",
        "#from dataset import OnlineTransformDataset_crm as OnlineTransformDataset\n",
        " \n",
        "#from util.logger import BoardLogger\n",
        "#from util.model_saver import ModelSaver\n",
        "#from util.hyper_para import HyperParameters\n",
        "#from util.log_integrator import Integrator\n",
        "#from util.metrics_compute_crm import compute_loss_and_metrics, iou_hooks_to_be_used\n",
        "#from util.image_saver_crm import vis_prediction\n",
        "\n",
        "import time\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Parse command line arguments\n",
        "para = HyperParameters()\n",
        "para = para.parse()\n",
        "\n",
        "\n",
        "\n",
        "# Logging\n",
        "if para['id'].lower() != 'null':\n",
        "    long_id = '%s_%s' % (para['id'],datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S'))\n",
        "else:\n",
        "    long_id = None\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "logger = BoardLogger(long_id)\n",
        "logger.log_string('hyperpara', str(para))\n",
        "\n",
        "print('CUDA Device count: ', torch.cuda.device_count())\n",
        "\n",
        "# Construct model\n",
        "model = CRMNet(backend='resnet50')\n",
        "model = nn.DataParallel(\n",
        "        model.cuda(), device_ids=[0] #[0,1]\n",
        "    )\n",
        "\n",
        "#if para['load'] is not None:\n",
        "    #model.load_state_dict(torch.load(para['load']))\n",
        "optimizer = optim.Adam(model.parameters(), lr=para['lr'], weight_decay=para['weight_decay'])\n",
        "\n",
        "\n",
        "duts_tr_dir = os.path.join('data', 'DUTS-TR')\n",
        "duts_te_dir = os.path.join('data', 'DUTS-TE')\n",
        "ecssd_dir = os.path.join('data', 'ecssd')\n",
        "msra_dir = os.path.join('data', 'MSRA_10K')\n",
        "\n",
        "\n",
        "#root_dir = \"/content/drive/MyDrive/multiclass-seg/cityscapes\"\n",
        "\n",
        "\n",
        "#train_dataset = OnlineTransformDataset_crm(root_dir, method=1, perturb=False)\n",
        "\n",
        "\n",
        "#fss_dataset = OnlineTransformDataset_crm(os.path.join('data', 'fss'), method=0, perturb=True)\n",
        "\n",
        "duts_tr_dataset = OnlineTransformDataset_crm(duts_tr_dir, method=1, perturb=True)\n",
        "duts_te_dataset = OnlineTransformDataset_crm(duts_te_dir, method=1, perturb=True)\n",
        "\n",
        "#ecssd_dataset = OnlineTransformDataset_crm(ecssd_dir, method=1, perturb=True)\n",
        "msra_dataset = OnlineTransformDataset_crm(msra_dir, method=1, perturb=True)\n",
        "\n",
        "####print('DUTS-TR dataset size: ', len(duts_tr_dataset))\n",
        "####print('DUTS-TE dataset size: ', len(duts_te_dataset))\n",
        "####print('MSRA-10K dataset size: ', len(msra_dataset))\n",
        "\n",
        "train_dataset = ConcatDataset([duts_tr_dataset, duts_te_dataset, msra_dataset]) #[fss_dataset, duts_tr_dataset, duts_te_dataset, ecssd_dataset, msra_dataset]\n",
        "\n",
        "##################train_dataset = ConcatDataset([ duts_tr_dataset, duts_te_dataset, msra_dataset]) #[fss_dataset, duts_tr_dataset, duts_te_dataset, ecssd_dataset, msra_dataset]\n",
        "\n",
        "print('Total training size: ', len(train_dataset))\n",
        "\n",
        "# For randomness: https://github.com/pytorch/pytorch/issues/5059\n",
        "def worker_init_fn(worker_id): \n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "\n",
        "# Dataloaders, multi-process data loading\n",
        "train_loader = DataLoader(train_dataset, para['batch_size'], shuffle=True, num_workers=8,\n",
        "                            worker_init_fn=worker_init_fn, drop_last=True, pin_memory=True)\n",
        "\n",
        "sobel_compute = SobelComputer()\n",
        "\n",
        "# Learning rate decay scheduling\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, para['steps'], para['gamma'])\n",
        "\n",
        "saver = ModelSaver(long_id)\n",
        "report_interval = 50\n",
        "save_im_interval = 800\n",
        "memory_chunk = 50176\n",
        "\n",
        "total_epoch = int(para['iterations']/len(train_loader) + 0.5)\n",
        "print('Actual training epoch: ', total_epoch)\n",
        "\n",
        "train_integrator = Integrator(logger)\n",
        "train_integrator.add_hook(iou_hooks_to_be_used)\n",
        "total_iter = 0\n",
        "last_time = 0\n",
        "for e in range(total_epoch):\n",
        "    np.random.seed() # reset seed\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Train loop\n",
        "    model = model.train()\n",
        "    for im, seg, gt, crm_data in train_loader:\n",
        "        im, seg, gt = im.cuda(), seg.cuda(), gt.cuda() # [12, 3, 224, 224] [12, 1, 224, 224] [12, 1, 224, 224]\n",
        "        for k, v in crm_data.items():\n",
        "            crm_data[k] = v.cuda()\n",
        "\n",
        "        total_iter += 1\n",
        "        if total_iter % 5000 == 0:\n",
        "            saver.save_model(model, total_iter)\n",
        "\n",
        "        images = {}\n",
        "        for i in range(0, seg.shape[-2]*seg.shape[-1], memory_chunk):\n",
        "            chunk_images = model(im, seg, coord=crm_data['coord'][:, i:i+memory_chunk, :], cell=crm_data['cell'][:, i:i+memory_chunk, :])\n",
        "            if 'pred_224' not in images.keys():\n",
        "                images = chunk_images\n",
        "            else:\n",
        "                for key in images.keys():\n",
        "                    images[key] = torch.cat((images[key], chunk_images[key]), axis=1)\n",
        "        for key in images.keys():\n",
        "            images[key] = images[key].view(images[key].shape[0], images[key].shape[1]//(seg.shape[-2]*seg.shape[-1]), *seg.shape[-2:])\n",
        "\n",
        "        images['im'] = im\n",
        "        images['seg'] = seg\n",
        "        images['gt'] = gt\n",
        "        sobel_compute.compute_edges(images)\n",
        "\n",
        "        loss_and_metrics = compute_loss_and_metrics(images, para)\n",
        "        train_integrator.add_dict(loss_and_metrics)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss1 = loss_and_metrics['total_loss'].float()\n",
        "        (loss1).backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if total_iter % report_interval == 0:\n",
        "            logger.log_scalar('train/lr', scheduler.get_lr()[0], total_iter)\n",
        "            train_integrator.finalize('train', total_iter)\n",
        "            train_integrator.reset_except_hooks()\n",
        "\n",
        "        # Need to put step AFTER get_lr() for correct logging, see issue #22107 in PyTorch\n",
        "        scheduler.step()\n",
        "\n",
        "        if total_iter % save_im_interval == 0:\n",
        "            predict_vis = vis_prediction(images)\n",
        "            logger.log_cv2('train/predict', predict_vis, total_iter)\n",
        "\n",
        "# Final save!\n",
        "saver.save_model(model, total_iter)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Z1nW0IdsQRes",
        "outputId": "d39cda33-c510-4432-f377-e46d93067c43"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn\\nfrom torch import optim\\nfrom torch.utils.data import DataLoader, ConcatDataset\\n\\n#from models.network.crm import CRMNet\\n#from models.sobel_op import SobelComputer\\n\\n#from dataset import OnlineTransformDataset_crm as OnlineTransformDataset\\n \\n#from util.logger import BoardLogger\\n#from util.model_saver import ModelSaver\\n#from util.hyper_para import HyperParameters\\n#from util.log_integrator import Integrator\\n#from util.metrics_compute_crm import compute_loss_and_metrics, iou_hooks_to_be_used\\n#from util.image_saver_crm import vis_prediction\\n\\nimport time\\nimport os\\nimport datetime\\n\\ntorch.backends.cudnn.benchmark = True\\n\\n# Parse command line arguments\\npara = HyperParameters()\\npara = para.parse()\\n\\n\\n\\n# Logging\\nif para[\\'id\\'].lower() != \\'null\\':\\n    long_id = \\'%s_%s\\' % (para[\\'id\\'],datetime.datetime.now().strftime(\\'%Y-%m-%d_%H:%M:%S\\'))\\nelse:\\n    long_id = None\\n\\n\\n\\n  \\nlogger = BoardLogger(long_id)\\nlogger.log_string(\\'hyperpara\\', str(para))\\n\\nprint(\\'CUDA Device count: \\', torch.cuda.device_count())\\n\\n# Construct model\\nmodel = CRMNet(backend=\\'resnet50\\')\\nmodel = nn.DataParallel(\\n        model.cuda(), device_ids=[0] #[0,1]\\n    )\\n\\n#if para[\\'load\\'] is not None:\\n    #model.load_state_dict(torch.load(para[\\'load\\']))\\noptimizer = optim.Adam(model.parameters(), lr=para[\\'lr\\'], weight_decay=para[\\'weight_decay\\'])\\n\\n\\nduts_tr_dir = os.path.join(\\'data\\', \\'DUTS-TR\\')\\nduts_te_dir = os.path.join(\\'data\\', \\'DUTS-TE\\')\\necssd_dir = os.path.join(\\'data\\', \\'ecssd\\')\\nmsra_dir = os.path.join(\\'data\\', \\'MSRA_10K\\')\\n\\n\\n#root_dir = \"/content/drive/MyDrive/multiclass-seg/cityscapes\"\\n\\n\\n#train_dataset = OnlineTransformDataset_crm(root_dir, method=1, perturb=False)\\n\\n\\n#fss_dataset = OnlineTransformDataset_crm(os.path.join(\\'data\\', \\'fss\\'), method=0, perturb=True)\\n\\nduts_tr_dataset = OnlineTransformDataset_crm(duts_tr_dir, method=1, perturb=True)\\nduts_te_dataset = OnlineTransformDataset_crm(duts_te_dir, method=1, perturb=True)\\n\\n#ecssd_dataset = OnlineTransformDataset_crm(ecssd_dir, method=1, perturb=True)\\nmsra_dataset = OnlineTransformDataset_crm(msra_dir, method=1, perturb=True)\\n\\n####print(\\'DUTS-TR dataset size: \\', len(duts_tr_dataset))\\n####print(\\'DUTS-TE dataset size: \\', len(duts_te_dataset))\\n####print(\\'MSRA-10K dataset size: \\', len(msra_dataset))\\n\\ntrain_dataset = ConcatDataset([duts_tr_dataset, duts_te_dataset, msra_dataset]) #[fss_dataset, duts_tr_dataset, duts_te_dataset, ecssd_dataset, msra_dataset]\\n\\n##################train_dataset = ConcatDataset([ duts_tr_dataset, duts_te_dataset, msra_dataset]) #[fss_dataset, duts_tr_dataset, duts_te_dataset, ecssd_dataset, msra_dataset]\\n\\nprint(\\'Total training size: \\', len(train_dataset))\\n\\n# For randomness: https://github.com/pytorch/pytorch/issues/5059\\ndef worker_init_fn(worker_id): \\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\\n\\n# Dataloaders, multi-process data loading\\ntrain_loader = DataLoader(train_dataset, para[\\'batch_size\\'], shuffle=True, num_workers=8,\\n                            worker_init_fn=worker_init_fn, drop_last=True, pin_memory=True)\\n\\nsobel_compute = SobelComputer()\\n\\n# Learning rate decay scheduling\\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, para[\\'steps\\'], para[\\'gamma\\'])\\n\\nsaver = ModelSaver(long_id)\\nreport_interval = 50\\nsave_im_interval = 800\\nmemory_chunk = 50176\\n\\ntotal_epoch = int(para[\\'iterations\\']/len(train_loader) + 0.5)\\nprint(\\'Actual training epoch: \\', total_epoch)\\n\\ntrain_integrator = Integrator(logger)\\ntrain_integrator.add_hook(iou_hooks_to_be_used)\\ntotal_iter = 0\\nlast_time = 0\\nfor e in range(total_epoch):\\n    np.random.seed() # reset seed\\n    epoch_start_time = time.time()\\n\\n    # Train loop\\n    model = model.train()\\n    for im, seg, gt, crm_data in train_loader:\\n        im, seg, gt = im.cuda(), seg.cuda(), gt.cuda() # [12, 3, 224, 224] [12, 1, 224, 224] [12, 1, 224, 224]\\n        for k, v in crm_data.items():\\n            crm_data[k] = v.cuda()\\n\\n        total_iter += 1\\n        if total_iter % 5000 == 0:\\n            saver.save_model(model, total_iter)\\n\\n        images = {}\\n        for i in range(0, seg.shape[-2]*seg.shape[-1], memory_chunk):\\n            chunk_images = model(im, seg, coord=crm_data[\\'coord\\'][:, i:i+memory_chunk, :], cell=crm_data[\\'cell\\'][:, i:i+memory_chunk, :])\\n            if \\'pred_224\\' not in images.keys():\\n                images = chunk_images\\n            else:\\n                for key in images.keys():\\n                    images[key] = torch.cat((images[key], chunk_images[key]), axis=1)\\n        for key in images.keys():\\n            images[key] = images[key].view(images[key].shape[0], images[key].shape[1]//(seg.shape[-2]*seg.shape[-1]), *seg.shape[-2:])\\n\\n        images[\\'im\\'] = im\\n        images[\\'seg\\'] = seg\\n        images[\\'gt\\'] = gt\\n        sobel_compute.compute_edges(images)\\n\\n        loss_and_metrics = compute_loss_and_metrics(images, para)\\n        train_integrator.add_dict(loss_and_metrics)\\n\\n        optimizer.zero_grad()\\n        loss1 = loss_and_metrics[\\'total_loss\\'].float()\\n        (loss1).backward()\\n        optimizer.step()\\n\\n        if total_iter % report_interval == 0:\\n            logger.log_scalar(\\'train/lr\\', scheduler.get_lr()[0], total_iter)\\n            train_integrator.finalize(\\'train\\', total_iter)\\n            train_integrator.reset_except_hooks()\\n\\n        # Need to put step AFTER get_lr() for correct logging, see issue #22107 in PyTorch\\n        scheduler.step()\\n\\n        if total_iter % save_im_interval == 0:\\n            predict_vis = vis_prediction(images)\\n            logger.log_cv2(\\'train/predict\\', predict_vis, total_iter)\\n\\n# Final save!\\nsaver.save_model(model, total_iter)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "def save_to_drive(source_path):\n",
        "  #print(source_path)\n",
        "  shutil.copy(source_path,\"/content/drive/MyDrive/multiclass-seg/CRM-models/CRM_unedited\")"
      ],
      "metadata": {
        "id": "NvfxohIZ87xE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "save_path = os.path.join('.', 'weights')\n",
        "\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(save_path, 'model_%s' % 8000)\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print('Model saved to %s.' % model_path)\n",
        "save_to_drive(model_path)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fin827U93dJY",
        "outputId": "b3c78257-803e-4b80-f5c1-4b72fc18ccdd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\nimport os\\n\\nsave_path = os.path.join('.', 'weights')\\n\\nos.makedirs(save_path, exist_ok=True)\\n\\nmodel_path = os.path.join(save_path, 'model_%s' % 8000)\\ntorch.save(model.state_dict(), model_path)\\nprint('Model saved to %s.' % model_path)\\nsave_to_drive(model_path)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import timeit\n",
        "\n",
        "model_temp = torch.load(\"/content/drive/MyDrive/multiclass-seg/CRM-models/CRM_unedited/model_8000\")\n",
        "model = torch.nn.DataParallel(CRMNet(backend = 'resnet50'))\n",
        "model.load_state_dict(model_temp)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id): \n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/multiclass-seg/cityscapes\"\n",
        "dataset = OnlineTransformDataset_Second(root_dir, method=1, perturb=False)\n",
        "loader = DataLoader(dataset, 1, shuffle=True, num_workers=8,\n",
        "                            worker_init_fn=worker_init_fn, drop_last=True, pin_memory=True)\n",
        "\n",
        "memory_chunk = 50176\n",
        "\n",
        "counter = 0\n",
        "\n",
        "final_images =[]\n",
        "for im, seg, gt, crm_data in loader:\n",
        "        im, seg, gt = im.cuda(), seg.cuda(), gt.cuda() # [12, 3, 224, 224] [12, 1, 224, 224] [12, 1, 224, 224]\n",
        "        for k, v in crm_data.items():\n",
        "            crm_data[k] = v.cuda()\n",
        "\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        final_seg = torch.empty(1, 1, 220, 440)\n",
        "        \n",
        "\n",
        "        #start = timeit.default_timer()\n",
        "        for c in range(1, seg.shape[1]):\n",
        "\n",
        "\n",
        "\n",
        "          class_images = []\n",
        "          input_seg = seg[0, c]\n",
        "          input_seg = input_seg.unsqueeze(0).unsqueeze(0)\n",
        "          \n",
        "          images = {}\n",
        "          for i in range(0, seg.shape[-2]*seg.shape[-1], memory_chunk):\n",
        "              \n",
        "              chunk_images = model(im, input_seg, coord=crm_data['coord'][:, i:i+memory_chunk, :], cell=crm_data['cell'][:, i:i+memory_chunk, :])\n",
        "              if 'pred_224' not in images.keys():\n",
        "                  images = chunk_images\n",
        "              else:\n",
        "                  for key in images.keys():\n",
        "                      images[key] = torch.cat((images[key], chunk_images[key]), axis=1)\n",
        "          for key in images.keys():\n",
        "            images[key] = images[key].view(images[key].shape[0], images[key].shape[1]//(seg.shape[-2]*seg.shape[-1]), *seg.shape[-2:])\n",
        "\n",
        "            if key == 'pred_224':\n",
        "\n",
        "              converted_matrix = np.where(images[key].cpu().detach().numpy() <= 0.5, 0, 1)\n",
        "              converted_matrix = torch.from_numpy(converted_matrix)\n",
        "              final_seg = torch.cat((final_seg, converted_matrix), axis=1)\n",
        "\n",
        "        #stop = timeit.default_timer()\n",
        "\n",
        "        #print(\"time: \" + str(stop-start))\n",
        "\n",
        "\n",
        "        np_matrix = final_seg.detach().numpy()\n",
        "        zero_rows = np.where(~np_matrix.any(axis=(2, 3)))[0]\n",
        "        np_matrix[0, zero_rows, :, :] = 1\n",
        "\n",
        "        j_max_index = np.argmax(np_matrix, axis=1)\n",
        "        I = np.eye(20)\n",
        "        new_matrix = I[j_max_index]\n",
        "        new_matrix = np.transpose(new_matrix, (0, 3, 1, 2))\n",
        "        bool_matrix = (~np.all(np_matrix==0, axis=1)).astype(int)\n",
        "        new_matrix *= bool_matrix\n",
        "\n",
        "        new_matrix = np.argmax(new_matrix, axis=1)\n",
        "        new_matrix = torch.from_numpy(new_matrix).to(\"cuda\")\n",
        "        gtmax = gt.argmax(1).to(\"cuda\")\n",
        "        seg = seg.argmax(1).to(\"cuda\")\n",
        "\n",
        "        old_iou = compute_tensor_iou(seg, gtmax)\n",
        "        new_iou = compute_tensor_iou(new_matrix, gtmax)\n",
        "\n",
        "\n",
        "        print(\"old iou:\" + str(old_iou))\n",
        "        print(\"new iou: \" + str(new_iou))\n",
        "        print(\"iou gain: \" + str(new_iou-old_iou))\n",
        "\n",
        "\"\"\"\n",
        "images['im'] = im\n",
        "images['seg'] = seg\n",
        "images['gt'] = gt\n",
        "sobel_compute.compute_edges(images)\n",
        "\n",
        "loss_and_metrics = compute_loss_and_metrics(images, para)\n",
        "train_integrator.add_dict(loss_and_metrics)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w4wd9JKHzEsY",
        "outputId": "af2e07dc-86f4-4ad8-c7c0-27c1b1196cc4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASPP_4level\n",
            "2975 images found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-18-c5429e971289>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt = torch.tensor(gt, dtype=torch.float)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-18-c5429e971289>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt = torch.tensor(gt, dtype=torch.float)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-18-c5429e971289>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt = torch.tensor(gt, dtype=torch.float)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-18-c5429e971289>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt = torch.tensor(gt, dtype=torch.float)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-18-c5429e971289>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt = torch.tensor(gt, dtype=torch.float)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-18-c5429e971289>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt = torch.tensor(gt, dtype=torch.float)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-18-c5429e971289>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt = torch.tensor(gt, dtype=torch.float)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-18-c5429e971289>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt = torch.tensor(gt, dtype=torch.float)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "old iou:tensor([0.9138], device='cuda:0')\n",
            "new iou: tensor([0.4593], device='cuda:0')\n",
            "iou gain: tensor([-0.4544], device='cuda:0')\n",
            "old iou:tensor([0.6994], device='cuda:0')\n",
            "new iou: tensor([0.3651], device='cuda:0')\n",
            "iou gain: tensor([-0.3343], device='cuda:0')\n",
            "old iou:tensor([0.8475], device='cuda:0')\n",
            "new iou: tensor([0.2769], device='cuda:0')\n",
            "iou gain: tensor([-0.5706], device='cuda:0')\n",
            "old iou:tensor([0.8244], device='cuda:0')\n",
            "new iou: tensor([0.4861], device='cuda:0')\n",
            "iou gain: tensor([-0.3383], device='cuda:0')\n",
            "old iou:tensor([0.8429], device='cuda:0')\n",
            "new iou: tensor([0.1481], device='cuda:0')\n",
            "iou gain: tensor([-0.6948], device='cuda:0')\n",
            "old iou:tensor([0.7822], device='cuda:0')\n",
            "new iou: tensor([0.2493], device='cuda:0')\n",
            "iou gain: tensor([-0.5329], device='cuda:0')\n",
            "old iou:tensor([0.8655], device='cuda:0')\n",
            "new iou: tensor([0.2831], device='cuda:0')\n",
            "iou gain: tensor([-0.5825], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-91ef40170377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m               \u001b[0mchunk_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrm_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coord'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmemory_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrm_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cell'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmemory_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0;34m'pred_224'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                   \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a480a859c92e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seg, coord, cell, inter_s8, inter_s4)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mfeat_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_coord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}